{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook performs the following tasks:\n",
    "\n",
    "3.1 [Explore binary features](#3.1)\n",
    "\n",
    "\n",
    "3.2 [Build classifier](#3.2)\n",
    "\n",
    "Kush did tasks 3.1-3.2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data tools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ML tools\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data_train_clean1.csv')\n",
    "test = pd.read_csv('data_test_clean1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Explore binary features\n",
    "<a id='3.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hope is that there's a binary feature that closely maps whether or not a permit is issued. We explore these below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, these are the baseline accuracies computed from the constant model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline training set accuracy = 0.9252513826043238\n",
      "Baseline test set accuracy     = 0.9234288587229764\n"
     ]
    }
   ],
   "source": [
    "baseline_tr = max(sum(train['Issued or not'] == 1), sum(train['Issued or not'] == 0)) / train.shape[0]\n",
    "baseline_te = max(sum(test['Issued or not'] == 1), sum(test['Issued or not'] == 0)) / test.shape[0]\n",
    "\n",
    "print(f\"Baseline training set accuracy = {baseline_tr}\")\n",
    "print(f\"Baseline test set accuracy     = {baseline_te}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10178, 144636],\n",
       "       [  1716,   2590]], dtype=int64)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(train['Site Permit'], train['Issued or not'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10522, 143049],\n",
       "       [  1372,   4177]], dtype=int64)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(train['Structural Notification'], train['Issued or not'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11448, 132581],\n",
       "       [   446,  14645]], dtype=int64)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(train['Fire Only Permit'], train['Issued or not'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binary features above are hardly indicative of issuance. By inspection they're all close to the constant model. \n",
    "We believe this is due to the fact that permits aren't unissued because of the underlying building modification, but rather something more technical in the permit process. \n",
    "\n",
    "Our hypothesis is that permits are rejected because they don't provide enough technical information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_estimated_cost = np.array(train['Estimated Cost'].isna())\n",
    "missing_revised_cost   = np.array(train['Revised Cost'].isna())\n",
    "missing_existing_type  = np.array(train['Existing Construction Type'].isna())\n",
    "missing_proposed_type  = np.array(train['Proposed Construction Type'].isna())\n",
    "missing_existing_units = np.array(train['Existing Units'].isna())\n",
    "missing_proposed_units = np.array(train['Proposed Units'].isna())\n",
    "missing_existing_use   = np.array(train['Existing Use'].isna())\n",
    "missing_proposed_use   = np.array(train['Proposed Use'].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7098, 147220],\n",
       "       [  4796,      6]], dtype=int64)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(missing_revised_cost, train['Issued or not'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Build classifier\n",
    "<a id='3.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all feature spaces that were attempted are shown. These include permit type, zipcode, and various numerical variables. The margin $C$ was hand-tuned on a validation set but made no improvements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_labels(data):\n",
    "    missing_revised_cost = np.array(data['Revised Cost'].isna())\n",
    "    X = missing_revised_cost.reshape(missing_revised_cost.shape + (1,))\n",
    "    y = np.array(data['Issued or not'])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, y_tr = features_labels(train)\n",
    "X_te, y_te = features_labels(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy = 0.9553544494720966\n",
      "Test accuracy     = 0.955052790346908\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(C=1.0, gamma='auto')\n",
    "clf.fit(X_tr, y_tr)\n",
    "\n",
    "train_acc = clf.score(X_tr, y_tr) \n",
    "test_acc  = clf.score(X_te, y_te)\n",
    "print(f\"Training accuracy = {train_acc}\")\n",
    "print(f\"Test accuracy     = {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data dictionary doesn't clarify when exactly \"Revised Cost\" is entered during the construction process. It's not something that's always entered after a permit is issued since we'd see 100% accuracy in that case. Based on this extremely simple model, we can say that permits should be updated with a revised cost during the application process in order to increase the chance of it being issued.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
